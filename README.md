# Sentence-Level-AI-Generated-Text-Detection
SeqXGPT is a model for sentence level AI generated text detection. 
The implementation is based on the paper [SeqXGPT: Sentence-Level AI-Generated Text Generation](https://arxiv.org/abs/2310.08903)

## SeqXGPT

SeqXGPT is an advanced technique for sentence-level AI-generated text detection. The model architecture comprises of the following three parts: 

1. Perplexity Extraction
2. Feature Encoder
3. Linear Classification Layer

## Setting up the project

1. Clone this repository and change directory to the project directory.
   
2. Set up the virtual environment using:
```bash
python3 -m venv myenv
source venv/bin/activate
```
3. Install the dependencies using:
```bash
pip install -r requirements.txt
```
   
## Datasets
Each dataset contains six files. Within each dataset folder, based on the source of AI-generated sentences in the document, they are organized into different files. Below is SeqXGPT-Bench.

### SeqXGPT-Bench

A sentence-level AI-generated text (AIGT) detection dataset used for the study of fine-grained AIGT detection.

**data format:**

```python
{
    "text": "Media playback is unsupported on your device 21 June 2013 Last updated at 12:31 BST The Market Hall Cinema in Brynmawr used to be run by the local council but when it announced its funding would stop last month, work began to find a way to keep it going. Thanks to the efforts of a group of local volunteers, the cinema has been saved and reopened under a new community initiative. The group, called \"Brynmawr Foundation\", raised enough funds to take over the lease of the building and purchase new equipment. They plan to show a mix of classic and new films, as well as host events and live performances. The Market Hall Cinema has been an important part of the town's history since it first opened in 1894, and this new initiative ensures that it will continue to be a valuable resource for the community.", 
    "prompt_len": 254, 
    "label": "gpt2"
}
```
**`text`** refers to an entire document.

​	**`prompt_len`** marks the boundary between the sentences generated by humans and those generated by AI. The first `prompt_len` characters of the input `text`, i.e., *text[:prompt\_len]*, are the sentences generated by humans, while the rest are generated by a particular language model.

​	**`label`** is the label for each sentence, and there are six types of labels in total: `gpt2`, `gptneo`, `gptj`, `llama`, `gpt3re`, `human`.

## Feature Extraction

We extract the features for each input file for each of the three LLMs using dataset/features_extractor_from_data.py file. 

#### We used A100 GPU available with Google Colab's Pro version for this part as feature extraction for the LLMs has high VRAM requirements.

**To extract the features from any input data file, please use the following script:**

```bash
# --model: [GPT2, GPTJ, GPTNeo]
python3 -m dataset.features_extractor_from_data --input_file input.jsonl --output_file output.jsonl --get_features --model model_name
```

***You can store the features for each LLM (GPT2, GPTJ, GPTNeo) for each of the six input files. Let's say for the file gpt2_lines.jsonl you get the gpt2_lines_gpt2_features.jsonl, gpt2_lines_gptj_features.jsonl, gpt2_lines_gptneo_features.jsonl files and store them under dataset/features/gpt2_lines***

So each of these feature files will have the features stored in jsonl format where each json line has the format:

```python
{
  "wordwise_loss_list": [[[0.0, 6.484006881713867, 6.484006881713867, 10.804593086242676, 10.804593086242676, 6.741743087768555,.........]],
  "text": ""high - salt has been shown to play a role in the pathogenesis of autoimmune disease . in this study , we investigated the effect of high - salt on the production of inflammatory mediators by arpe-19 cells and the possible mechanisms involved . arpe-19 cells were cultured with lps in dmem to which extra nacl had been added ( 20  mm and 40  mm ) . a) The   addition of sodium chloride ( 1 mmol/ l ) did not influence their growth and (b) lps treated with salt ( 40 mm ) significantly (p<0.01) induced an IC 50 value of 11 nmol TNFa/mL. The addition of lps in dmem for the addition of the salt ( 40 mm ) resulted in a significant increase in the IC 50 in the range of 25 - 33 nmol TNFa/ml.\n\nThe mechanism by which the      salt influences arpe-19 cells immunity is not known, however, the authors suggested that the increase in the production of TNFa may result from lysosome-remodeling effect of lps. In this study, arpe-19 cells have been treated for 24 hours with lps at different concentrations which resulted to significantly different results. lps-treated arpe-19 cells grew as       compared to untreated cells in the range of 40.3 - 57.3% of the initial culture volume. This increase in proliferation indicates that lps acts via the activation of arpe-19 cells immune mechanism.\n\nLps has also been recently used for the anti-tumour effect in cancer cell line. in this study, we found that Lps can bind to EBOV-1 and -2 and promote the cell        death", 
  "prompt_len": 347, 
  "label": "gpt2",
  "label_int": 0
}
```
Then we merge the wordwise_loss_lists for each feature file and get the complete features file for a particular input file (gpt2_lines_features.jsonl in this case) using the following script

```bash
python3 -m dataset.combine_features --directory dataset/gpt2_lines --output_file gpt2_lines_features.jsonl
```
Then we save the features.jsonl file for each input file in a directory dataset/features

A sample json line in this features.jsonl file post combining would look like:
```python
{
  "wordwise_loss_list": [[0.0, 6.484006881713867, 6.484006881713867, 10.804593086242676, .........], [7.503107070922852, 7.478128910064697, 4.111625671386719, 4.111625671386719,....], [0.1008826345205307, 0.1008826345205307, 3.0469260215759277, 3.046926259994507, 0.04809051379561424,....]],
  "text": ""high - salt has been shown to play a role in the pathogenesis of autoimmune disease . in this study , we investigated the effect of high - salt on the production of inflammatory mediators by arpe-19 cells and the possible mechanisms involved . arpe-19 cells were cultured with lps in dmem to which extra nacl had been added ( 20  mm and 40  mm ) . a) The   addition of sodium chloride ( 1 mmol/ l ) did not influence their growth and (b) lps treated with salt ( 40 mm ) significantly (p<0.01) induced an IC 50 value of 11 nmol TNFa/mL. The addition of lps in dmem for the addition of the salt ( 40 mm ) resulted in a significant increase in the IC 50 in the range of 25 - 33 nmol TNFa/ml.\n\nThe mechanism by which the      salt influences arpe-19 cells immunity is not known, however, the authors suggested that the increase in the production of TNFa may result from lysosome-remodeling effect of lps. In this study, arpe-19 cells have been treated for 24 hours with lps at different concentrations which resulted to significantly different results. lps-treated arpe-19 cells grew as       compared to untreated cells in the range of 40.3 - 57.3% of the initial culture volume. This increase in proliferation indicates that lps acts via the activation of arpe-19 cells immune mechanism.\n\nLps has also been recently used for the anti-tumour effect in cancer cell line. in this study, we found that Lps can bind to EBOV-1 and -2 and promote the cell        death", 
  "prompt_len": 347, 
  "label": "gpt2",
  "label_int": 0
}
```

***These feature files will be used as the dataset for the SeqXGPT model***

## Training the SeqXGPT model

```bash
python3 -m driver --data_path dataset/features --train_path dataset/train/train.jsonl --test_path dataset/test/test.jsonl --split_dataset --num_train_epochs=100 --gpu=gpu_count
```
We can change other args like train_ratio, learning rate, warmup_ratio, weight_decay in the above script.

## Evaluation

```bash
# Add --test_content as arg if you want content level evaluation else it will be sentence level evaluation by default
python3 -m driver --train_path dataset/train/train.jsonl --test_path dataset/test/test.jsonl --gpu=gpu_count --inference 
```

## Results

























